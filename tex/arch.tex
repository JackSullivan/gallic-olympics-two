\documentclass[11pt]{article}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{enumerate}
\usepackage{fullpage}
\begin{document}
\title{LAB 1}
\author{Jack Sullivan and Ari Kobren}
\maketitle

\section{Main Components}
As per the assignment, our architecture is built around three central
components: one \emph{DBServer} and two \emph{FrontEndServer}s. These
components are responsible for handling most of the requests from all
of clients.

To make it easy to interact with these three components, we have also
included a \emph{RequestRouter}.  This is a single router node that
accepts requests from all clients and evenly distributes them. The
\emph{RequestRouter} supervises 5 \emph{RequestWorkers} each of whom
is arbitrarily assigned a \emph{FrontEndServer}. When the
\emph{RequestRouter} receives a standard message, it distributes the
message to one of its workers who forwards it to the appropriately
(effectively load balancing). The load balacing follows a round-robin
task assignment schedule.  The \emph{RequestRouter} can also accept
\emph{DBWrite} messages from the \emph{CacafonixClient} (see previous
submission).  When it receives a \emph{DBWrite} message, the router
directly forwards the messages to one of the \emph{FrontEndServer}s.

\subsection{Leader Election}
\label{subsec:leader}

\subsection{Clock Synchronization}
\label{subsec:sync}

\subsection{Vector Clocks}
\label{subsec:vector}
We implement causally ordered messages using vector clocks.  To do
this, both front-end servers and the back-end server are able to
accept \emph{SendInOrder} messages (each of which wraps a single
\emph{payload} message).  When a server receives a \emph{SendInOrder}
message, the server updates its clock, extracts the payload and wraps
it, along with it's own vector clock, in a new \emph{TimedMessage}
message.  The \emph{TimedMessage} is then broadcast to all of the
other servers.

When a server receives a \emph{TimedMessage} it adds that message to
its internal message queue. After that, the server begins trying to
process all of the messages in its queue.  To maintain a causal
ordering, the server, $s_l$, will only process a \emph{TimedMessage},
$\mathcal{M}$, from server, $s_r$, if
\begin{align*}
 \forall i \ne r \quad \mathcal{C}[i] \ge \mathcal{M}[i] \\
 \mathcal{C}[r] = \mathcal{M}[r] - 1
\end{align*}

This means that a server will only process a message if it has
received all other messages from that sender and at least all messages
that sender has received from everyone else.  If the server cannot
find a valid message to process, it waits for the next message. In
this way, we make the assumption that the servers will all receive all
messages (however, the messages may be out of causal order).

To process a message, a server simply unwraps the payload of a
\emph{TimedMessage} and processes it normally.

\subsection{Previously Built Components}
With our modular design, we were able to easily incorporate components
(messages and classes) that we built for Lab 1.  Specifically, we used
(and slightly modified): \emph{EventRoster}, \emph{TeamRoster},
\emph{Event}, \emph{EventMessage}, \emph{CacofonixListener},
\emph{EventSubscription}, \emph{MedalTally}, \emph{TabletClients}
and \emph{EventScore}.

\section{System Runtime Workflow}
We start our system by running two \emph{FrontEndServer}s and the
\emph{DBServer} all on different processes.  We also start a process
for \emph{CacafonixClient} and $n$ \emph{TabletClients}.  When the system
fully comes up, we run leader election (Section ~\ref{subsec:leader})
and clock synchronization (Section ~\ref{subsec:sync}).

The clients continually send \emph{EventScore} and \emph{MedalTally}
requests as in the previous assignment. As before, clients receive
responses to their requests and print them out. The difference is now
all of these requests are sent to the \emph{RequestRouter} which
forwards the requests to the \emph{FrontEndServer}s who timestamp the
messages (i.e. wrap in \emph{TimedMessage} objects).  As described
above (Section ~\ref{subsec:vector}) the messages are causally ordered
using vector clocks.  When the \emph{DBServer} receives these
requests, it orders them and keeps every hundreth for the raffle.
\emph{CacafonixClient} sends updates periodically as in the previous
assignment.

\section{Design, Bottlenecks and Potential Improvements}
Like before, we modularized our design to make debuggin easier.  This
also makes our system more resistent to failures.

That said, with more time, we'd make our system more fault
tolerant.  Although leader election occurs when the nodes initially
come up, we could improve our system by setting other nodes to watch
in case some of the nodes go down.  Then new nodes could be respawned
and leader election/clock synchronization run again.

Additionally, we could improve our system by making new clients join
the system and register themsleves with the routers.  This would
complicate things significatly: vector clock causal order would be
more compex (to introduce a new node midway through); additionally,
there would be more logic (i.e. handshaking to make sure clients were
connected properly to the rest of the system.  Because there are more
independent components in our implementation of Lab 2 (as opposed to
Lab 1) this would be a bit more complicate.

Lastly, we'd like to connect more of our components through routers
with special routing logic (i.e. round-robin) than with pointers. We
noticed that passing messages through routers is much more efficient
and failure tolerant than when we simply pass them independently from
node to node (as we do in some of our transactions).

One bottleneck in our overall design is that the
\emph{FrontEndServer}s are single threaded.  This means that they have
to reason about their vector clocks and respond to requests on a single
thread.  We could improve our implementation by multithreading our servers.

\section{Results}
In our first experiment, we ran our code using 1, 5 and 10 clients
each with a rate of one request per .5 seconds and measured the min,
max and average response times.

\begin{tabular}{c|c|c|c}
  NUM CLIENTS & MIN & MAX & AVG \\
  \hline
  1  & 0.01s & 0.02s & 0.01385s \\
  5  & 0.01s & 0.03s & 0.01342s \\
  10 & 0.01s & 0.12s & 0.01475s \\
\end{tabular}

In our second experiment we ran our code using 5 clients with .01, .1
and 0.4 seconds between requests and measured the min, max and
average response times.


\begin{tabular}{c|c|c|c}
  REQUEST FREQ. & MIN & MAX & AVG \\
  \hline
  0.01s & 0.01s & 0.3s  & 0.11667s \\
  0.1s  & 0.01s & 0.07s & 0.01266s \\
  0.4s  & 0.01s & 0.06s & 0.01369s \\
\end{tabular}

As we observe, adding more clients seems to increase average
latency.  However, there is no real difference between having 1 client
and 5 clients.  We hypothesize that adding a tremendous number of
clients would have a serious effect on the latency.

Also, we observe that having the clients make requests at higher
frequencies severely affects the latency.  Again, there is not much
difference between a request rate of 0.4s and 0.1s however, when we
increase the request rate by an order of magnitude we see that the
average latency per request increases substantially.

\section{Software}
Like Lab 1, we've built our system on top of the Akka \emph{actors}
library.  This library provides a hierarchical message passing
architecture for the Scala programming language.

\section{How to Run}

\end{document}
